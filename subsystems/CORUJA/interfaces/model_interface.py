"""Defines the abstract interface for interacting with Large Language Models (LLMs).

This module contains:
- Abstract Base Class `ModelInterface` defining the required methods for LLM interaction.
- Pydantic model `ModelResponse` for standardizing the output from LLM calls.
- Custom exceptions related to model interaction errors.
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field

# --- Custom Exceptions ---


class CorujaException(Exception):
    """Base exception for CORUJA subsystem errors."""

    pass


class ModelConfigurationError(CorujaException):
    """Error related to model configuration (e.g., missing API key, invalid settings)."""

    pass


class ModelApiException(CorujaException):
    """Generic error during API interaction with the model (e.g., network issue, server error)."""

    pass


class ModelRateLimitError(ModelApiException):
    """Specific error for hitting API rate limits."""

    pass


class ModelSafetyError(ModelApiException):
    """Error raised when the model flags content for safety reasons (input or output)."""

    pass


class ModelTimeoutError(ModelApiException):
    """Error raised when the API call times out."""

    pass


# --- Standardized Response Object --- \


class TokenUsage(BaseModel):
    """Structure for reporting token usage, if provided by the model API."""

    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None


class ModelResponse(BaseModel):
    """Standardized response object from a model execution via ModelInterface.

    Attributes:
        text: The primary text response generated by the model.
        finish_reason: Reason the model stopped generating (varies by model).
        token_usage: Details about token consumption.
        safety_ratings: Model's safety assessment (structure varies by model).
        model_name: Identifier of the model used.
        error: Description of an error if one occurred during generation.
        raw_response: Original response object from the API for debugging.
    """

    text: str = Field(..., description="The primary text response generated by the model.")
    finish_reason: Optional[str] = Field(
        None,
        description=(
            "Reason the model stopped generating (e.g., 'stop', 'length', "
            "'content_filter', 'tool_calls')."
        ),
    )
    token_usage: Optional[TokenUsage] = Field(
        None, description="Details about token consumption, if available."
    )
    safety_ratings: Optional[Any] = Field(
        None, description="Model's safety assessment, structure varies by model/API."
    )
    model_name: Optional[str] = Field(
        None,
        description="Identifier of the model that generated the response (e.g., 'gemini-1.5-pro').",
    )
    error: Optional[str] = Field(
        None,
        description=(
            "Description of an error if one occurred during generation and was handled internally."
        ),
    )
    raw_response: Optional[Any] = Field(
        None,
        description=(
            "Optional: The raw response object from the underlying API for debugging "
            "or accessing non-standard fields."
        ),
    )

    class Config:
        extra = "allow"  # Allow extra fields if needed, but direct access is preferred.


# --- Abstract Base Class for Model Interfaces ---


class ModelInterface(ABC):
    """Abstract Base Class for LLM interactions within CORUJA.

    Defines the standard contract for sending prompts to different LLMs
    and receiving responses in a consistent `ModelResponse` format.
    Concrete implementations must handle model-specific API calls, parameter mapping,
    authentication, and error translation.
    """

    @abstractmethod
    async def execute_prompt(
        self, prompt: str, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None
    ) -> ModelResponse:
        """Executes a given prompt against the configured LLM asynchronously.

        Args:
            prompt: The fully rendered prompt string to send to the model.
            params: A dictionary of parameters controlling generation
                    (e.g., temperature, max_output_tokens, top_p). Concrete
                    implementations MUST map these generic names or handle
                    model-specific parameter names passed within this dict.
            context: Optional dictionary containing additional context about the
                     request (e.g., {'pdd_id': 'xyz', 'user_session': 'abc',
                     'trace_id': '123'}) for logging, tracing, or fine-tuned behavior.

        Returns:
            A ModelResponse object containing the generated text and metadata.

        Raises:
            ModelConfigurationError: If the model interface is improperly configured
                                     (e.g., missing API key during setup or call).
            ModelRateLimitError: If the underlying API rate limit is exceeded.
            ModelSafetyError: If the model blocks the request/response due to safety filters.
            ModelTimeoutError: If the API call times out.
            ModelApiException: For other API-related errors during execution (
                e.g., connection issues, invalid requests based on API specifics
            ).
            CorujaException: For other unexpected errors within the interface implementation.
        """
        pass

    # Optional: Consider adding a synchronous version or helper if needed,
    # but the primary interface should be async for I/O bound operations.

    # Optional: A method to get model capabilities or supported parameters?
    # def get_capabilities(self) -> Dict[str, Any]:
    #     pass


# --- Define potential custom exceptions for AI interactions ---


class AIError(Exception):
    """Base class for exceptions related to AI model interactions."""

    pass


class AICommunicationError(AIError):
    """Raised when there is a network or API communication issue."""

    pass


class AIConfigurationError(AIError):
    """Raised when the AI model interface is missing configuration or is misconfigured."""

    pass


class AIResponseError(AIError):
    """Raised when the AI model returns an error response or unexpected content."""

    pass
